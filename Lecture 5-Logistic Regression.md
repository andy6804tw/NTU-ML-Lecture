# Logistic Regression

## å›é¡§
ä¸Šä¸€ç¯€èª²æœ€å¾Œæœ‰æåˆ°ï¼Œæˆ‘å€‘è¦æ‰¾ä¸€å€‹æ©Ÿç‡ posterior probabilityï¼Œç•¶æ©Ÿç‡ P(X1|x) å¤§æ–¼ 0.5 å‰‡è¼¸å‡º C1 åä¹‹ C2ã€‚å¦‚æœæ¡ç”¨çš„æ˜¯ Gaussian æ©Ÿç‡åˆ†ä½ˆï¼Œæˆ‘å€‘å¯ä»¥èªªé€™å€‹ posterior probability å°±æ˜¯ ğœ(ğ‘§)ã€‚è€Œ z=wâ‹…x+b ç•¶æ¡ç”¨å…¶å®ƒçš„æ©Ÿç‡åˆ†ä½ˆçš„è©±ï¼Œæœ€çµ‚å¾—åˆ°çš„çµæœä¹Ÿæ˜¯å·®ä¸å¤šçš„ã€‚ ä¸‹æ¨™ w,b ä»£è¡¨é€™å€‹ function set æ˜¯å–æ±ºæ–¼ w,b ä»–å€‘æ˜¯é€éè¨“ç·´å¾—åˆ°çš„ä¸€çµ„åƒæ•¸ï¼Œä¸åŒçš„ w,b å°±æœƒå¾—åˆ°ä¸åŒçš„ functionã€‚æ–¼æ˜¯æˆ‘å€‘å¯ä»¥èªª fw,b(x) å³ç‚º posteriror probabilityã€‚

![](https://i.imgur.com/u33dd6r.png)

## Step 1: Function Set
å¦‚æœä»¥åœ–åƒåŒ–è¡¨ç¤ºæœƒé•·é€™æ¨£ã€‚æˆ‘å€‘çš„ function æœ‰å…©çµ„åƒæ•¸ï¼Œä¸€çµ„æ˜¯ w æˆ‘å€‘ç¨±ç‚º weightï¼Œå¦ä¸€å€‹å¸¸æ•¸ b ç¨±ç‚º biasã€‚å°‡ I å€‹è¼¸å…¥åˆ†åˆ¥ä¹˜ä¸Š w å†åŠ ä¸Š b å°±å¯ä»¥å¾—åˆ° zï¼Œç„¶å¾Œé€šéä¸€å€‹ sigmoid function å¾—åˆ°çš„è¼¸å‡ºå°±æ˜¯ posterior probabilityã€‚ä»¥ä¸Šå°±æ˜¯ä¸€å€‹ Logistic Regression çš„é‹ä½œæ©Ÿåˆ¶ã€‚

![](https://i.imgur.com/Qg1RR78.png)

## æ¯”è¼ƒ Logistic Regression èˆ‡ Linear Regression æ¨¡å‹
é€™è£å°‡ Logistic Regression èˆ‡ Linear Regression å…©è€…åšä¸€ä¸‹æ¯”è¼ƒã€‚Logistic Regression æŠŠæ¯ä¸€å€‹ç‰¹å¾µéƒ½ä¹˜ä¸Šä¸€å€‹ w ä¸¦åŠ ä¸Š bã€‚æœ€å¾Œå†é€šé sigmoid function ç„¶å¾Œè¼¸å‡ºã€‚å› ç‚ºè¼¸å‡ºæœ‰ç¶“éä¸€å€‹ sigmoid functionï¼Œå› æ­¤è¼¸å‡ºä¸€å®šä»‹æ–¼ 0~1 ä¹‹é–“ã€‚è‡³æ–¼ Linear Regression ä¸€æ¨£æŠŠç‰¹å¾µä¹˜ä¸Š w å†åŠ ä¸Š b å°±ç›´æ¥è¼¸å‡ºï¼Œå› æ­¤è¼¸å‡ºå¯ä½¿æ˜¯ä»»ä½•ä¸€å€‹æ•¸å€¼ã€‚å…©è‘—æœ€å¤§çš„å·®åˆ¥å°±æ˜¯ç·šæ€§è¿´æ­¸æ¨¡å‹çš„è¼¸å‡ºä¸¦ç„¡ç¶“éä¸€å€‹ sigmoid functionã€‚

![](https://i.imgur.com/4cwIy4P.png)

## Step 2: Goodness of a Function
å‡è¨­è¨“ç·´è³‡æ–™é›†æ˜¯ä¾æ“šæˆ‘å€‘æ‰€å®šç¾©çš„ Posterior Probability æ‰€ç”¢ç”Ÿçš„ï¼Œå› æ­¤åªè¦çµ¦äº†ä¸€çµ„çš„ w,b å°±ç›¸å°çš„æ±ºå®šäº† Posterior Probabilityï¼Œå°±å¯ä»¥è¨ˆç®—é€™çµ„ w èˆ‡ b ç”¢ç”Ÿè©²è¨“ç·´è³‡æ–™é›†çš„æ©Ÿç‡ã€‚

![](https://i.imgur.com/JqbkZlR.png)

æˆ‘å€‘ç›®æ¨™æ˜¯è¦æ‰¾ä¸€çµ„ w å’Œ b æœ€å¤§åŒ– Likelihoodï¼Œèª¿æ•´æ•¸å­¸å¼å°‡åŸæœ¬è¦æ‰¾ä¸€çµ„ w,b æœ€å¤§åŒ– L(w,b) è®Šæ›´ç‚º minâˆ’lnL(w,b) ã€‚å– log çš„å¥½è™•å°±æ˜¯ç›¸ä¹˜è®Šç›¸åŠ ï¼Œè€ŒåŠ ä¸Šè² è™Ÿå°±å¾æœ€æ‰¾æœ€å¤§è®Šæˆæ‰¾æœ€å°ã€‚æˆ‘å€‘å°‡ C1,C2 ä»¥ 0, 1 ä¾†è¡¨ç¤ºå°±å¯ä»¥æ–¹ä¾¿çš„è¨ˆç®—ã€‚

![](https://i.imgur.com/utwsjnr.png)

æˆ‘å€‘åŸæœ¬æ˜¯è¦æ‰¾æœ€å¤§ Likelihood çš„ functionï¼Œæ›å€‹è§’åº¦å¾ L(w,b) èª¿æ•´ç‚º âˆ’lnâ¡L(w,b)ï¼Œä¸¦å°‡é¡åˆ¥ C1,C2 ä»¥ 1,0 ä¾†è¡¨ç¤ºã€‚å› æ­¤æˆ‘å€‘å¯ä»¥å°‡æœ€å°åŒ–çš„ç›®æ¨™å¯«æˆä¸€å€‹å‡½æ•¸ï¼Œå³ï¼š

![](https://i.imgur.com/ES7oVGD.png)

è€ŒåŠ ç¸½çš„éƒ¨ä»½å…¶å¯¦æ˜¯å…©å€‹ Bernoulli distribution çš„ Cross entropy(äº¤å‰ç†µ)ã€‚Cross entropy ä»£è¡¨ p å’Œ q é€™å…©å€‹åˆ†ä½ˆæœ‰å¤šæ¥è¿‘ï¼Œå¦‚æœå…©å€‹åˆ†ä½ˆä¸€æ¨£çš„è©±ï¼Œé‚£æ‰€å¾—å³ç‚º0ã€‚ 

![](https://i.imgur.com/uODlKjo.png)

æˆ‘å€‘å¯ä»¥ç™¼ç¾é€™å€‹ loss function å…¶å¯¦å°±æ˜¯ä¸€å€‹ binary cross entropy lossã€‚

![](https://i.imgur.com/gUcCGdb.png)

## æ¯”è¼ƒ Logistic Regression èˆ‡ Linear Regression æå¤±å‡½æ•¸
åœ¨ Logistic Regression ä¸­æˆ‘å€‘å®šç¾©çš„æå¤±å‡½æ•¸æ˜¯è¦å»æœ€å°åŒ–çš„å°è±¡æ˜¯æ‰€æœ‰è¨“ç·´è³‡æ–™ cross entropy çš„ç¸½å’Œã€‚æˆ‘å€‘å¸Œæœ›æ¨¡å‹çš„è¼¸å‡ºè¦è·Ÿç›®æ¨™ç­”æ¡ˆè¦è¶Šæ¥è¿‘è¶Šå¥½ã€‚è€Œ Linear Regression ç›¸å°å–®ç´”åªéœ€è¦å°‡çœŸå¯¦ç­”æ¡ˆèˆ‡æ¨¡å‹é æ¸¬è¼¸å‡ºåšä¸€å€‹ Square Error è¨ˆç®—å³å¯ã€‚

![](https://i.imgur.com/av1JisT.png)

## Step 3: Find the best function
ç¬¬ä¸‰æ­¥æ˜¯å°‹æ‰¾ä¸€çµ„æœ€å¥½çš„åƒæ•¸ï¼Œä½¿å¾— loss èƒ½å¤ æœ€ä½ã€‚å› æ­¤é€™è£¡æ¡ç”¨æ¢¯åº¦ä¸‹é™ (Gradient Descent) ä¾†æœ€å°åŒ–äº¤å‰ç†µ (Cross Entropy)ã€‚ä½œæ³•ä¸Šå³æ˜¯è¨ˆç®— âˆ’lnL(w,b) å°å„ wi çš„åå¾®åˆ†ã€‚é‚£ lnL(w,b) å¦‚ä½•å° w åå¾®åˆ†å‘¢ï¼Ÿæˆ‘å€‘çŸ¥é“ f æ˜¯å— z é€™å€‹è®Šæ•¸å½±éŸ¿ã€‚ç„¶è€Œé€™å€‹ z æ˜¯å¾ w, x, b æ‰€ç”¢ç”Ÿçš„ã€‚å› æ­¤æˆ‘å€‘å¯ä»¥å°‡å¾®åˆ†å¼å­è¡¨ç¤ºå¦‚ä¸‹ï¼š

![](https://i.imgur.com/btwnYl9.png)

ğœ•ğ‘§/ğœ•ğ‘¤ğ‘– ä»£è¡¨çš„æ˜¯ xiï¼Œå› ç‚º z åªæœ‰ä¸€é …èˆ‡ wi*xi æœ‰é—œã€‚

![](https://i.imgur.com/xpV7e1R.png)

é‚£å‰é¢é€™é …çš„å¼å­å°‡ f(x) æ›æˆ ğœ(z) ä¸¦ä½œå¾®åˆ†ã€‚ğœ(z) æ˜¯ sigmoid functionï¼Œä»–çš„å¾®åˆ†æ˜¯ ğœ(z) * (1-ğœ(z))ã€‚

![](https://i.imgur.com/eZGcKbJ.png)

å› æ­¤æˆ‘å€‘æœ€å¾Œå¾—åˆ°  (1-ğœ(z))*xiï¼Œå…¶ä¸­ ğœ(z)  å°±æ˜¯ f(x)ã€‚ä»¥ä¸Šå¾®åˆ†å¾Œçš„é€šå¼ç‚º Cross Entropy å¼å­ä¸­çš„å·¦é …ã€‚

![](https://i.imgur.com/SaEDeuU.png)

Cross Entropy å¼å­ä¸­çš„å³é …ä¹Ÿæ˜¯ä¸€æ¨£çš„æ¨å°æ–¹å¼ã€‚å¯ä»¥æ‹†æˆ ln(1-f(x)) å…ˆå° z åšåå¾®åˆ†ï¼Œw å†å° z åšåå¾®åˆ†ã€‚è€Œ ğœ•ğ‘§/ğœ•ğ‘¤ğ‘– ä»£è¡¨çš„æ˜¯ xiã€‚æœ€å¾Œå¾—åˆ° ğœ(z)*xiã€‚

![](https://i.imgur.com/dmvPFch.png)

æœ€å¾Œå°‡ä¸Šé¢æ¨å°çš„å¼å­å¸¶å…¥ä¹‹å¾Œï¼Œå°‡è®Šæ•¸ç§»é …ä¸¦å±•é–‹å¼å­ã€‚ä¸¦æŠµéŠ·ä¸€æ¨£çš„ç®—å¼å¾Œï¼Œå¾—åˆ°ä¸€å€‹å¾ˆç›´è§€çš„çµæœã€‚æœ€çµ‚å° loss function åå¾®åˆ†å¾Œçš„çµæœæ˜¯è »å®¹æ˜“ç†è§£çš„ã€‚å¾—åˆ°çš„çµæœå°±æ˜¯æ¯ä¸€é …éƒ½æ˜¯è² çš„ Ground true(y) æ¸›å»æ¨¡å‹æ‰€é æ¸¬çš„å€¼å¾Œä¸¦ä¹˜ä¸Š x çš„æ¯ä¸€é …ã€‚

å¦‚æœç”¨ Gradient Descent æ›´æ–°å®ƒçš„è©±å°±æ˜¯ç•¶å‰çš„æ¬Šé‡ wi æ¸›å» Learning Rate(Î·) ä¹˜ä¸Šæ‰€æœ‰çš„è¨“ç·´é›†å¸¶å…¥åœ¨ loss åå¾®åˆ†çš„å¼å­ã€‚æ¬Šé‡çš„æ›´æ–°å–æ±ºæ–¼ä¸‰ä»¶äº‹ï¼š

1. Learning Rate(Î·) è‡ªå·±è¨­ç½®
2. xi ä¾†è‡ªæ–¼è¨“ç·´è³‡æ–™é›†ä¸­çš„æ¯å€‹è¼¸å…¥ç‰¹å¾µ
3. y^ ä»£è¡¨è©²ç­†è³‡æ–™çš„çœŸå¯¦ç­”æ¡ˆ


![](https://i.imgur.com/JKAYvah.png)

## æ¯”è¼ƒ Logistic Regression èˆ‡ Linear Regression åƒæ•¸æ›´æ–°æ–¹å¼
æœ€å¾Œå¯ä»¥ç™¼ç¾ï¼ŒLinear Regression èˆ‡ Logistic Regression åœ¨æ›´æ–°çš„éƒ¨ä»½ä¹Ÿæ˜¯åŸ·è¡Œä¸€æ¨£çš„æ­¥é©Ÿï¼Œä¸åŒçš„éƒ¨ä»½åœ¨æ–¼ Logistic çš„ y^ æ˜¯0æˆ–1ï¼Œè€Œ Linear çš„ y^ æ˜¯ä»»æ„å¯¦æ•¸ã€‚

![](https://i.imgur.com/MRPcDPa.png)

å¦‚æœä»¥ Linear Regression çš„æå¤±å‡½æ•¸ä¾†æ±‚ Logistic Regression çš„è©±ï¼Œä»¥ Class1 åœ¨ y^=1 çš„æ™‚å€™ï¼Œä¸è«– fw,b(x) æ˜¯0æˆ–1æ‰€å¾—çš„åå¾®åˆ†éƒ½æ˜¯0ã€‚é€™æ„å‘³è‘—å¥—å…¥å…¬å¼å¾Œç®—å‡ºä¾†é›¢ç›®æ¨™å¾ˆè¿‘å’Œç›®æ¨™å¾ˆé çš„æ™‚å€™ç®—å‡ºä¾†çš„å¾®åˆ†éƒ½æœƒç­‰æ–¼0ã€‚

![](https://i.imgur.com/GYiaSv9.png)

å¦‚æœæˆ‘å€‘å°‡åƒæ•¸è®ŠåŒ–å° total loss ç•«æˆåœ–çš„è©±ã€‚å¯ä»¥ç™¼ç¾åˆ†é¡å•é¡Œä½¿ç”¨ Square  Error çš„è©±åœ¨è·é›¢ç›®æ¨™å¾ˆé å’Œå¾ˆè¿‘æ™‚æ¢¯åº¦éƒ½æ˜¯å¾ˆå¹³å¦çš„ã€‚è€Œä½¿ç”¨é©ç•¶çš„ Cross  Entropy ï¼Œè·é›¢ç›®æ¨™è¶Šé æ¢¯åº¦å°±è¶Šå¤§å› æ­¤åƒæ•¸æ›´æ–°çš„å¹…åº¦ä¹Ÿæœƒæ¯”è¼ƒå¤§ï¼Œå¯ä»¥æœ‰å€‹æ˜ç¢ºçš„ç›®æ¨™å¹«åŠ©æˆ‘å€‘å¿«é€Ÿæ”¶æ–‚åˆ°è°·åº•ã€‚

![](https://i.imgur.com/1f95VPJ.png)

## Discriminative vs Generative
Logistic Regression çš„æ–¹æ³•ç¨±ä½œ Discriminativeã€‚è€Œç”¨é«˜æ–¯åˆ†ä½ˆä¾†æè¿° posterior probability é€™ç¨®æ–¹å¼ç¨±ä½œ Generative æ–¹æ³•ã€‚å¯¦éš›ä¸Šå…©è€…æ¨¡å‹çš„ function set æ˜¯ä¸€æ¨£çš„ã€‚åªè¦åšæ©Ÿç‡æ¨¡å‹çš„æ™‚å€™æŠŠ covariance matrix è¨­å®šç‚ºå…±äº«çš„ï¼Œå…¶å¯¦å…©è€…æ¨¡å‹æ˜¯ä¸€æ¨£çš„ ğœ(ğ‘¤ âˆ™ ğ‘¥ + ğ‘)ã€‚

å¦‚æœæ˜¯ Discriminative æ–¹æ³•å°±æ˜¯é€éæ¢¯åº¦ä¸‹é™æ³•å°‡ w èˆ‡ b æ‰¾å‡ºä¾†ã€‚å¦‚æœæ˜¯ Generative æ–¹æ³•çš„è©±æˆ‘å€‘æœƒå»ç®— ğœ‡1, ğœ‡2, Î£âˆ’1 ä¸¦å°‡ w èˆ‡ b ç®—å‡ºä¾†ã€‚

![](https://i.imgur.com/lUFvAWa.png)

Discriminative èˆ‡ Generative é›–ç„¶æ˜¯åŒä¸€å€‹æ¨¡å‹è¨ˆç®—ï¼Œä½†æ˜¯æœ€å¾Œçš„ function set æœƒæ˜¯ä¸ä¸€æ¨£çš„çµæœã€‚å…¶å› ç‚ºæ˜¯å…©è€…åšäº†ä¸åŒçš„å‡è¨­ï¼Œæ‰€ä»¥è·Ÿå»åŒä¸€çµ„è¨“ç·´è³‡æ–™æ‰¾å‡ºä¾†çš„åƒæ•¸æœƒæ˜¯ä¸ä¸€æ¨£çš„ã€‚

> åœ¨æ–‡ç»ä¸Š Discriminative æœƒæ¯” Generative é‚„ä¾†å¾—å¥½ã€‚

![](https://i.imgur.com/bgbbsIu.png)

## Generative vs. Discriminative
Generative æ¨¡å‹å‡è¨­è³‡æ–™ä¾†è‡ªæ–¼ä¸€å€‹æ©Ÿç‡æ¨¡å‹ï¼Œä¸¦åšäº†æŸäº›å‡è¨­ã€‚ä»–åœ¨è³‡æ–™æ¨£æœ¬å°‘çš„ç‹€æ³ä¸‹ï¼ŒGenerative çš„å„ªå‹¢æ˜¯æœƒç„¡è¦– data è€Œéµå¾ªå®ƒå…§å¿ƒè‡ªå·±çš„å‡è¨­ï¼Œæˆ–æ˜¯è³‡æ–™ä¸­å¸¶æœ‰é›œè¨Šã€‚åœ¨é€™äº›æƒ…æ³ä¸‹æœ‰æ™‚å€™çµæœæœƒæ˜¯æ¯” Discriminative é‚„ä¾†å¾—å¥½ã€‚

åœ¨åš Discriminative çš„æ™‚å€™æ˜¯ç›´æ¥å‡è¨­ä¸€å€‹ posterior probabilityï¼Œç„¶å¾Œå»æ‰¾è£¡é¢çš„åƒæ•¸ã€‚è€Œåœ¨åš Generative çš„æ™‚å€™æœƒå°‡ function æ‹†æˆ Priors èˆ‡ class-dependent probabilities é€™å…©é …ã€‚é€™ä»¶äº‹æƒ…åœ¨èªéŸ³è¾¨è­˜ä¸­æ˜¯å€‹é‡è¦é—œéµã€‚


![](https://i.imgur.com/1bq95JD.png)